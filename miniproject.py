# -*- coding: utf-8 -*-
"""Miniproject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D72Zelpflb5Kp4ome4bdvWFxFzfMoSOF
"""

import cv2
import torch
import matplotlib.pyplot as plt

!pip install timm

# Download the MiDas
model_type = "MiDaS_small"
midas = torch.hub.load('intel-isl/MiDas',model_type,trust_repo=False)
midas.to('cpu')
midas.eval()

#Load transforms to resize and normalize the image for large or small model

transforms = torch.hub.load("intel-isl/MiDaS", "transforms", trust_repo=False)
transform = transforms.small_transform

from IPython.display import display, Javascript
from google.colab.output import eval_js
from base64 import b64decode

def take_photo(filename='photo.jpg', quality=0.8):
  js = Javascript('''
    async function takePhoto(quality) {
      const div = document.createElement('div');
      const capture = document.createElement('button');
      capture.textContent = 'Capture';
      div.appendChild(capture);

      const video = document.createElement('video');
      video.style.display = 'block';
      const stream = await navigator.mediaDevices.getUserMedia({video: true});

      document.body.appendChild(div);
      div.appendChild(video);
      video.srcObject = stream;
      await video.play();

      // Resize the output to fit the video element.
      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

      // Wait for Capture to be clicked.
      await new Promise((resolve) => capture.onclick = resolve);

      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);
      stream.getVideoTracks()[0].stop();
      div.remove();
      return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
  display(js)
  data = eval_js('takePhoto({})'.format(quality))
  binary = b64decode(data.split(',')[1])
  with open(filename, 'wb') as f:
    f.write(binary)
  return filename

from IPython.display import Image
try:
  filename = take_photo()
  print('Saved to {}'.format(filename))

  # Show the image which was just taken.
  display(Image(filename))
except Exception as err:
  # Errors will be thrown if the user does not have a webcam or if they do not
  # grant the page permission to access it.
  print(str(err))

img = cv2.imread(filename)
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

input_batch = transform(img).to('cpu')

with torch.no_grad():
    prediction = midas(input_batch)

    prediction = torch.nn.functional.interpolate(
        prediction.unsqueeze(1),
        size=img.shape[:2],
        mode="bicubic",
        align_corners=False,
    ).squeeze()

output = prediction.cpu().numpy()

plt.imshow(output)
# Function to convert depth values to real-world distances
def depth_to_distance(depth_value, depth_scale):
    # depth_scale is a scaling factor to convert depth to real-world units
    real_distance = depth_value * depth_scale
    return real_distance
# take input for  the cordinate
x =int (input("horizontal coordinate"))
y= int (input("vertical coordinate"))
# Example usage
 # Coordinates of the pixel for which we want the distance
depth_value = output[y, x]
depth_scale = 1.0  # Example scale factor; you need to calibrate this for your specific use case
distance = depth_to_distance(depth_value, depth_scale)

print(f"The distance at pixel ({x}, {y}) is: {distance} units")

distance_in_meters = distance * 0.0002645833

print(f"The distance at pixel ({x}, {y}) is: {distance_in_meters} meters")

distance_in_cm = distance_in_meters*100
print(f"The distance at pixel ({x}, {y}) is: {distance_in_cm} cm")
